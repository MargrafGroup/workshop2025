{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Notebook\n",
    "\n",
    "This notebook provides some rough markdown comment guidelines for you to program your own invariant/equivariant NN.\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to import all the relevant packages.<br>\n",
    "Just come back and add what's missing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from e3nn import o3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Molecular Graph Representation**\n",
    "\n",
    "\n",
    "Let's begin by defining an object containing the relevant graph information, derived from an ASE atoms object.\n",
    "\n",
    "We define a lightweight wrapper class that extracts geometric and chemical information from an ASE atoms object.\n",
    "<br> <br>\n",
    "\n",
    "<details>\n",
    "<summary> <strong> Hint </strong> (click me) </summary>\n",
    "\n",
    "Define some `__init__`, taking the `ase.atoms`-object, and a function to build a neighbour list.\n",
    "- Stores atomic positions and element types\n",
    "- Builds a neighbor list using ASE (with periodicity and cutoff)\n",
    "- Computes normalized direction vectors for atom pairs\n",
    "- Encodes element types as consecutive indices (not atomic numbers)\n",
    "</details>\n",
    "\n",
    "<br> \n",
    "\n",
    "<details>\n",
    "<summary> <strong> Neighbor list hint </strong> (click me) </summary>\n",
    "You can use ase neighbor list as this\n",
    "```\n",
    "from ase.neighborlist import neighbor_list\n",
    "idx_i, idx_j, d_ij, r_ij, S = neighbor_list(\"ijdDS\",at,cutoff=cutoff,self_interaction=False)\n",
    "```\n",
    "- Encodes element types as consecutive indices (not atomic numbers)\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchAtoms():\n",
    "    def __init__(self,aseAtoms):\n",
    "        '''\n",
    "        Simple class containing molecular graph information based on an atoms object\n",
    "        '''\n",
    "        self.ase_mol = aseAtoms.copy()\n",
    "        self.xyz = torch.tensor(aseAtoms.get_positions())\n",
    "        self._build_neighbor_list()\n",
    "        self.elements = list(set(self.ase_mol.symbols))\n",
    "        self.n_elements = len(self.elements)\n",
    "        self.element_labels = torch.tensor([self.elements.index(sym) for sym in self.ase_mol.symbols]) # Elements are labeled consecutively not by atomic number (to match the index of the embedding)\n",
    "        self.n_atoms = len(aseAtoms)\n",
    "        \n",
    "    def _build_neighbor_list(self, cutoff=3.0):\n",
    "        at = self.ase_mol.copy()\n",
    "        at.center(vacuum=10)\n",
    "        idx_i, idx_j, d_ij, r_ij, S = neighbor_list(\"ijdDS\",at,cutoff=cutoff,self_interaction=False)\n",
    "        self.idx_i = torch.from_numpy(idx_i)\n",
    "        self.idx_j = torch.from_numpy(idx_j)\n",
    "        self.d_ij  = torch.from_numpy(d_ij).type(torch.FloatTensor)\n",
    "        self.r_ij  = torch.from_numpy(r_ij).type(torch.FloatTensor)\n",
    "        self.dir_ij = torch.nn.functional.normalize(self.r_ij,p=2,dim = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mini-batching multiple molecular systems**\n",
    "\n",
    "For later usage, let's also already predefine a batched version of our `TorchAtoms`, where we concanate relevant tensors.\n",
    "\n",
    "Define a container that merges several `TorchAtoms` objects into a single batched structure.\n",
    "\n",
    "Another option is to use dataloader from `torch`, however for our simple case it is not fully needed\n",
    "<br><br>\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary><strong> Hint </strong></summary>\n",
    "\n",
    "Each molecule has its own local atom indexing. To combine them into one large graph, indices must be shifted so they don’t overlap.\n",
    "    \n",
    "- Atom indices are adjusted via per-molecule offsets.\n",
    "- Neighbor and feature tensors are concatenated across molecules.\n",
    "- Each atom is tagged with a `mol_id` for later aggregation per system.\n",
    "\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List  # optional\n",
    "class BatchedTorchAtoms():\n",
    "    def __init__(self, \n",
    "                 mol_list: List[TorchAtoms]):\n",
    "        offsets = []\n",
    "        cum = 0\n",
    "        for mol in mol_list:\n",
    "            offsets.append(cum)\n",
    "            cum += mol.n_atoms\n",
    "        self.n_mols = len(mol_list)\n",
    "        self.idx_i = torch.cat([m.idx_i + off for m, off in zip(mol_list, offsets)])\n",
    "        self.idx_j = torch.cat([m.idx_j + off for m, off in zip(mol_list, offsets)])\n",
    "        self.d_ij  = torch.cat([m.d_ij  for m in mol_list])\n",
    "        self.r_ij  = torch.cat([m.r_ij  for m in mol_list])\n",
    "        self.dir_ij = torch.cat([m.dir_ij for m in mol_list])\n",
    "        self.element_labels = torch.cat([m.element_labels for m in mol_list])\n",
    "        self.n_elements = torch.unique(self.element_labels).numel() \n",
    "        mol_ids = [ #This generates indexes of which atom belong to which system\n",
    "            torch.full((m.n_atoms,), idx, dtype=torch.long) \n",
    "            for idx, m in enumerate(mol_list)\n",
    "        ]\n",
    "        self.mol_id = torch.cat(mol_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load and preprocess data**\n",
    "\n",
    "And load and prep objects of classes. Here we are providing data of water clusters calculated with FHI-aims on PBE0 level.\n",
    "\n",
    "We read a set of molecular structures (here: water clusters) and convert them into graph-based representations.\n",
    "\n",
    "<details>\n",
    "<summary><strong> Hint</strong></summary>\n",
    "\n",
    "This ASE syntax selects the first 10 configurations from a trajectory-style `.xyz` file.\n",
    "    \n",
    "- The `.xyz` file contains multiple frames; only the first 10 are read.\n",
    "- Each structure is wrapped as a `TorchAtoms` object.\n",
    "- A batched container combines all systems for model input.\n",
    "\n",
    "</details>\n",
    "\n",
    "**Extract graph features**\n",
    "\n",
    "Then, retrieve atomic types, pairwise distances, directions, and neighbor indices from the batched data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mols = read(\"waterAims.xyz@:10\",format=\"extxyz\")\n",
    "torch_mols = [TorchAtoms(mol) for mol in mols]\n",
    "batched_mols = BatchedTorchAtoms(torch_mols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read stuff from the batched_mols to test block before we will put everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_types = batched_mols.element_labels\n",
    "r_ij   = batched_mols.r_ij\n",
    "d_ij   = batched_mols.d_ij\n",
    "dir_ij = batched_mols.dir_ij\n",
    "idx_i  = batched_mols.idx_i\n",
    "idx_j  = batched_mols.idx_j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each ase.atoms obejct has following info and arrays. The important bits will be `dft_dipole`, `dft_hirshfeld` and `dft_energy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([k for k in mols[0].info])\n",
    "print([k for k in mols[0].arrays])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As long as we will do a one-batch training, the code will be the same, so feel free to use `batched_mols` or `torch_mols[0]` in your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this moment on, you are on your own. GL HF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invariant NN - SchNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Atom-type embeddings**\n",
    "\n",
    "The first step towards building the atomic representations is to match each element to a vector of dimension `n_features` via a so called embedding (basically just an optimizable look-up table).\n",
    "\n",
    "Now, map discrete atomic species to continuous feature vectors using a learnable embedding layer.\n",
    "<br><br>\n",
    "<details>\n",
    "<summary><strong> Hint</strong></summary>\n",
    "\n",
    "An `nn.Embedding` turns integer-labeled types into dense vectors that can be optimized during training.  \n",
    "This is analogous to word embeddings in NLP, but here used for element types.\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "Output shape: [n_atoms,n_features] (if you load 10 water molecules [240,n_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Edge features via radial basis functions**\n",
    "\n",
    "Next, we do the same thing with the edges of our graph (i.e. interatomic distances). Here we use some kind of radial basis functions (e.g. evenly spaced Gaussians).\n",
    "\n",
    "Code a class in order to project pairwise distances onto a fixed set of radial basis functions (RBFs), implemented as Gaussians.\n",
    "<br><br>\n",
    "<details>\n",
    "<summary><strong> Hint</strong></summary>\n",
    "\n",
    "Distances are continuous values. To represent them as input features, we expand them in a smooth basis (here: Gaussians)  \n",
    "centered along the range of expected distances. This helps the model capture distance-dependent interactions.\n",
    "Define a `__init__`-function for the class being a child of `nn.Modul`, and provide a `forward` method.\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encode edge distances**\n",
    "\n",
    "Apply the newly coded RBF expansion to pairwise distances to obtain edge features.\n",
    "\n",
    "<details>\n",
    "<summary><strong>Hint</strong></summary>\n",
    "\n",
    "Each edge gets a feature vector of length `n_rbf`.\n",
    "\n",
    "Input is in forward is d_ij we have above \n",
    "\n",
    "</details>\n",
    "\n",
    "Depends on the number of your distances in cut-off from our nl list aboe. In my settings, I am getting shape [1558,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Invariant Convolution**\n",
    "\n",
    "Finally, we need to define a module that allows us to couple the representations of the atoms (nodes) via the edges. To this end we can define a simple invariant convolution, based on the continuous filter convolution in SchNet. This works by projecting both edge and node embeddings to a common latent space (of dimension `filter_dimension`) and then computing a weighted sum over the neighboring atoms j as the message to atom i.\n",
    "\n",
    "We are going to use following equations:\n",
    "\n",
    "First since a lenght features of our vector are not equel to a lenght of our filter, we have to do simple linear (=dense) layer to expand feature vector into the feature filter (size of the embedding `feature_dimension`) input shape (`filter_dimension`)\n",
    "\n",
    "Convolution:\n",
    "$$\n",
    "\\mathbf{x}_i^{l+1}  = \\sum_{j=0}^{n_\\text{atoms}} \\mathbf{x}_j^l \\circ \\mathbf{R}^l(\\mathbf{r}_j - \\mathbf{r}_i)\n",
    "$$\n",
    "where R is the convolution filter generated via MLP from RBFs.\n",
    "<br><br>\n",
    "<details>\n",
    "<summary><strong>Hint</strong></summary>\n",
    "What does this convolution actually do?\n",
    "\n",
    "This operation lets atoms exchange information with their neighbors, modulated by the distance-based filter.  \n",
    "The edge-dependent filter `R_ij` is constructed from the RBF features and scaled by the sender node embedding.\n",
    "\n",
    "The final output updates each node’s feature by aggregating messages from its neighbors, preserving rotational invariance.\n",
    "\n",
    "</details>\n",
    "<br>\n",
    "<details>\n",
    "<summary><strong>Hint</strong></summary>\n",
    "\n",
    "Why `scatter_add`?\n",
    "\n",
    "We compute messages for each edge (from `j` to `i`),  \n",
    "then sum them per receiving atom `i` to get the new node representation.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvariantConvolution(nn.Module):\n",
    "    def __init__(self, \n",
    "                 feature_dimension, \n",
    "                 edge_dimension, \n",
    "                 filter_dimension, \n",
    "                 activation=torch.nn.functional.silu):\n",
    "        super(InvariantConvolution, self).__init__()\n",
    "        # TODO: Some linear function to get filter dimensions\n",
    "   \n",
    "    def forward(self,\n",
    "                x,\n",
    "                e_ij,\n",
    "                idx_i,\n",
    "                idx_j):\n",
    "        # TODO: Get Rij through our filter and do linear block to expand our features into the filter dimension. \n",
    "        # Do element multiplicatiosn between features (pick them with idx_j) and Rij\n",
    "\n",
    "        # return x # return x features of the dimension you put into the convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, set a `filter_dimension` and call the newly coded Invariant Convolution, and take a look at the output's shape.\n",
    "\n",
    "shape out with 10 water molecules [240,n_f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Full invariant message passing model `InvNet`**\n",
    "\n",
    "Now we can put everything together. \n",
    "\n",
    "Define a simple neural network that learns atomic representations via message passing based on rotationally invariant convolutions.\n",
    "<br><br>\n",
    "<details>\n",
    "<summary><strong>Hint</strong></summary>\n",
    "\n",
    "The model:\n",
    "\n",
    "- embeds element types into feature vectors\n",
    "- expands interatomic distances via radial basis functions\n",
    "- updates node features through multiple invariant interaction blocks\n",
    "- outputs per-atom scalar predictions via a readout layer\n",
    "\n",
    "</details>\n",
    "<br>\n",
    "<details>\n",
    "<summary><strong>Hint</strong></summary>\n",
    "What’s returned here?\n",
    "    \n",
    "The output is one scalar per atom. Later steps might aggregate these to get molecular properties.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple SO3-invariant representation \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dimension,\n",
    "        filter_dimension,\n",
    "        n_interactions, \n",
    "        radial_basis,\n",
    "        n_elements, \n",
    "        activation = torch.nn.functional.silu\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            feature_dimension: number of features to describe atomic environments.\n",
    "                This determines the size of each embedding vector; i.e. embeddings_dim.\n",
    "            n_interactions: number of interaction blocks.\n",
    "            lmax: maximum angular momentum of spherical harmonics basis\n",
    "            radial_basis: layer for expanding interatomic distances in a basis set\n",
    "            activation:\n",
    "        \"\"\"\n",
    "        super(InvNet, self).__init__()\n",
    "        # Do some self. saving here\n",
    "        \n",
    "        # initialize embeddings\n",
    "\n",
    "        # initialize linear mixing layers\n",
    "\n",
    "        # initialize invariant convolution\n",
    "\n",
    "        # initialize readout function. \n",
    "        # This is first place where you can experiment\n",
    "        \n",
    "\n",
    "    def forward(self, inputs: BatchedTorchAtoms):\n",
    "        \"\"\"\n",
    "        Compute atomic representations/embeddings.\n",
    "\n",
    "        Args:\n",
    "            inputs (dict of torch.Tensor): torchAtoms with input tensors.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: atom-wise representation.\n",
    "        \"\"\"\n",
    "        # get tensors from input \n",
    "\n",
    "        # Radial embedding\n",
    "\n",
    "        # Node embedding\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model initialization**\n",
    "\n",
    "Set the hyperparameters and instantiate your invariant message passing model.\n",
    "<br><br>\n",
    "<details>\n",
    "<summary><strong> Hint </strong></summary>\n",
    "\n",
    "Keep in mind:\n",
    "- Only two element types (likely H and O) are present in the water cluster dataset (`n_elements`)\n",
    "- set dimensions, `rbf` and `cutoff`$\\ldots$\n",
    "\n",
    "</details>\n",
    "\n",
    "Experiment with sizes. If you do not have Macbook Air from 2012 with 2 cores that are slower than CUDA cores, this is a good starting point:\n",
    "```\n",
    "feat_dim = 16\n",
    "filt_dim = 32\n",
    "n_rbf = 10\n",
    "cutoff = 3.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load reference labels**\n",
    "\n",
    "*CHARGE!*\n",
    "\n",
    "Extract the (DFT) Hirshfeld charges as training targets for each atom.\n",
    "<br><br>\n",
    "<details>\n",
    "<summary><strong> Hint</strong></summary>\n",
    "\n",
    "- Read the `.xyz` and use some class defined in the very beginning regarding `atoms`-objects$\\ldots$\n",
    "- Fill a list with the targets\n",
    "- Ensures the target tensor has shape `[N, 1]` to match the model output and support broadcasting.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training loop**\n",
    "\n",
    "*No Pain, no gain!*\n",
    "\n",
    "Train the model to predict atomic charges via mean squared error loss.\n",
    "<br><br>\n",
    "<details>\n",
    "<summary><strong>Hint</strong></summary>\n",
    "\n",
    "This simple example does not use dropout or batchnorm.\n",
    "Simply build a `for`-loop for the training.\n",
    "Define:\n",
    "- `n_epochs`\n",
    "- and optimizer\n",
    "- don't forget your model and loss\n",
    "- what do you have to call regarding the `grad` when in training ?\n",
    "- *optional but recommended: some tracking (e.g. every 10th epoch print epoch and loss)*\n",
    "</details>\n",
    "\n",
    "your loss should be something like \n",
    "```\n",
    "q = model(batched_mols)\n",
    "loss  = F.mse_loss(q, ref_charges)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction vs. reference**\n",
    "\n",
    "Compare predicted atomic charges to DFT reference using a scatter plot.\n",
    "<br><br>\n",
    "<details>\n",
    "<summary><strong>Hint:</strong> What would perfect predictions look like?</summary>\n",
    "\n",
    "All points lie on the diagonal: predicted charge = reference charge.<br>\n",
    "*Optional:* add this diagonal as visual reference.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not forget detach from the gradient and import to numpy as `ref_charges.detach().numpy()`\n",
    "\n",
    "With like 100 epochs, this can be relatively good prediction if you predicting on a training set. If it seems not totally stupid, it would be worth to rerun everything to get new random numbers, as we are doing the most naive approach, it may happen that our starting positions is not the greatest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Larger Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rerun with larger model**\n",
    "\n",
    "Increase model capacity by using more features, interactions, and wider RBF coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create batches**\n",
    "\n",
    "Split dataset into batches of 10 molecules and collect reference charges per batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training over batches**\n",
    "\n",
    "Loop over batches to compute loss and update model parameters.\n",
    "<br><br>\n",
    "<details>\n",
    "<summary><strong>Hint</strong></summary>\n",
    "\n",
    "Similar to before. Accumulate total loss to monitor progress across the entire dataset, not just one batch.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\ldots$ and visualize again via scatter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You’ve just built and trained your **first rotationally invariant neural network** for atomistic systems.\n",
    "\n",
    "From embeddings to message passing and radial filters – this pipeline forms the core of many modern models in computational chemistry and materials science.\n",
    "\n",
    "<details>\n",
    "<summary><strong>Where to go from here?</strong> (click)</summary>\n",
    "\n",
    "- Try training on larger datasets or predicting other atomic/molecular properties.   \n",
    "- Implement multiple convolution blocks or skip connections.  \n",
    "- Compare performance with baseline methods.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we will have time, we can return here and predict energy, but lets move to equivariance :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Equivariant NN - SO3Net "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we start by defining an embedding for the elements present in our dataset.  \n",
    "For this, we can copy the idea from the invariant model.\n",
    "  \n",
    "Extract the number of distinct elements from the `BatchedTorchAtoms` object and define an embedding with `feature_dimension=16`.\n",
    "\n",
    "<details>\n",
    "<summary><strong>Hint</strong></summary>\n",
    "What is the input to `nn.Embedding`?\n",
    "    \n",
    "You need the number of distinct atomic types (`n_elements`) as first argument, and the embedding dimension (`feature_dimension`) as second.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radial Basis & Edge Embedding\n",
    "\n",
    "Additionally, we again need a radial basis to represent the edge features.  \n",
    "We will reuse the `SimpleRBF` class from the invariant model.\n",
    " \n",
    "Using `SimpleRBF`, define a radial basis with:\n",
    "- 10 radial functions (`n_rbf = 10`)\n",
    "- cutoff radius of `3.0 Å`\n",
    "\n",
    "<details>\n",
    "    <summary><strong>Hint</strong></summary>\n",
    "What’s the input to `SimpleRBF`?\n",
    "    \n",
    "You need to specify `n_rbf` and `cutoff`. No need to change the `start` value.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This is a reminder cell, no need to reimplement this, use the cell from invariant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equivariant convolution (SO(3) tensor coupling)\n",
    "\n",
    "Now comes the critical part that separates **SO3Net** from simpler invariant models like **SchNet**:  \n",
    "the **equivariant convolution**. This operation couples edge and node features in a way that respects the transformation properties (irreducible representations, or *irreps*) under 3D rotations.\n",
    "\n",
    "In contrast to invariant convolutions, we must now ensure that **the character of each feature is preserved** throughout the network. This is what enables the model to remain **equivariant under SO(3) transformations**, i.e. to transform predictably under rotations.\n",
    "\n",
    "We encode the feature types via:\n",
    "\n",
    "`irrep_l = [\"x0e\",\"...\"]`\n",
    "\n",
    "These define the structure of the irreps for the different angular momentum orders `l`, alternating between even/odd parity (`e/o`).\n",
    "\n",
    "**Convolution workflow**\n",
    "\n",
    "This convolution works by projecting both node and edge features into a **shared latent space**\n",
    "and coupling them using spherical harmonics and a **tensor product** (via `e3nn.o3.FullyConnectedTensorProduct`).\n",
    "\n",
    "We are going to perform the following steps:\n",
    "\n",
    "1. **Edge MLP projection:**\n",
    "   Project the edge embeddings (i.e. RBF-expanded distances) through a fully connected layer with activation.\n",
    "   This transforms them into a suitable filter tensor `W_ij`.\n",
    "\n",
    "2. **Spherical harmonics computation:**\n",
    "   Use `e3nn.o3.spherical_harmonics` to encode the directional relation between atoms via `r_ij`.\n",
    "\n",
    "3. **Tensor product coupling:**\n",
    "   Perform the tensor product between node features `x_j`, the spherical harmonics, and the filter weights `W_ij`.\n",
    "   This step is **structure-aware** and **equivariant**.\n",
    "\n",
    "4. **Aggregation via scatter sum:**\n",
    "   Aggregate the messages sent to each atom `i` from its neighbors `j` using `scatter_add`.\n",
    "\n",
    "<details>\n",
    "<summary><strong>Hint</strong></summary>\n",
    "Why spherical harmonics?\n",
    "    \n",
    "Spherical harmonics serve as a rotational basis for directional interactions.\n",
    "By combining them with scalar edge features and node features via tensor product, we preserve equivariance.\n",
    "\n",
    "</details>\n",
    "<br>\n",
    "<details>\n",
    "<summary><strong>Hint</strong></summary>\n",
    " Why `FullyConnectedTensorProduct`?\n",
    "    \n",
    "This module generalizes matrix multiplication to `SO(3)`-equivariant representations.\n",
    "It defines how two irreps (e.g. node and direction) can be coupled into an output irrep.\n",
    "\n",
    "</details>\n",
    "<br>\n",
    "<details>\n",
    "<summary><strong>Task</strong>\n",
    "\n",
    "Implement an equivariant convolution module with the following interface: *(click to expand)* </summary>\n",
    "\n",
    "- **Constructor inputs:**\n",
    "\n",
    "  * `lmax`: maximum angular momentum of spherical harmonics\n",
    "  * `feature_dimension`: dimension per irrep block\n",
    "  * `edge_dimension`: input dimension of edge features\n",
    "  * `activation`: nonlinearity (e.g. `F.silu`)\n",
    "\n",
    "- **Forward inputs:**\n",
    "\n",
    "  * `x`: node features (with irreps defined by `lmax` and `feature_dimension`)\n",
    "  * `e_ij`: edge features (from RBF)\n",
    "  * `r_ij`: relative position vectors\n",
    "  * `idx_i`, `idx_j`: indices of atom pairs for message passing\n",
    "\n",
    "- **Steps in `forward`:**\n",
    "\n",
    "  * Compute spherical harmonics `Y_lm(r_ij)` using `o3.spherical_harmonics`\n",
    "  * Use an MLP (`Linear + activation`) on `e_ij` to get filter weights `W_ij`\n",
    "  * Apply a tensor product `x_j ⊗ Y_lm ⊗ W_ij` to compute messages\n",
    "  * Aggregate messages for each target atom `i` via `scatter_add`\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irrep_l = [\"x0e\",\"x1o\",\"x2e\",\"x3o\",\"x4e\"] \n",
    "\n",
    "class EquivariantConvolution(nn.Module):\n",
    "    def __init__(self, \n",
    "                 lmax,\n",
    "                 feature_dimension, \n",
    "                 edge_dimension, \n",
    "                 activation=F.silu):\n",
    "        super(EquivariantConvolution, self).__init__()\n",
    "        self.lmax = lmax\n",
    "        \n",
    "        # TODO: setup object with spherical_harmonics with provided lmax\n",
    "\n",
    "        # Prepare the irreducible representations\n",
    "        irrep_string = f\"{feature_dimension}{irrep_l[0]}\"\n",
    "        for l in range(lmax):\n",
    "            irrep_string += f\" + {feature_dimension}{irrep_l[l+1]}\"\n",
    "        # TODO: use prepared irrep_string with o3.Irreps to create an object of hidden_irreps \n",
    "        \n",
    "        # TODO: Define the tensor product and other neural network components\n",
    "        \n",
    "        # TODO: define linear function where you take edge_dimension and expand them into tensor product weight_numel\n",
    "        \n",
    "    def forward(self,\n",
    "                x,\n",
    "                e_ij,\n",
    "                r_ij,\n",
    "                idx_i,\n",
    "                idx_j):\n",
    "        # TODO: calculate spherical harmonics with irreps_sh, and get ylm variable\n",
    "\n",
    "        # TODO: linear riding from edges to filter size. Remember that here you are providing a MLP radial function for all combinations of l a s and \n",
    "        # the filter dimension is weight_numel\n",
    "\n",
    "        # use tensor product with x_j (see invariant), ylm. You will have to use Rij as weights of the tensor multiplication \n",
    "        \n",
    "        # do a scatter readout\n",
    "        # x = scatter_add(x_ij, idx_i, dim=0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using your `EquivariantConvolution`-class, define a convolution with a `feature_dimension` of 16 and `lmax` of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A first equivariant network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scalar-to-irrep expansion\n",
    "\n",
    "Before feeding scalar node embeddings into an `SO(3)`-equivariant network,  \n",
    "we need to lift them into the appropriate tensor shape for spherical harmonic coupling.\n",
    "\n",
    "Build a helper function that expands a `[N_atoms, N_features]` tensor into a padded form matching  \n",
    "the irreducible representation structure up to angular momentum `lmax`.\n",
    "\n",
    "<details>\n",
    "<summary><strong>Hint</strong></summary>\n",
    " Why pad with zeros?\n",
    "    \n",
    "The initial node features are scalars (`l=0`). To match the full SO(3) structure,  \n",
    "we zero-pad the higher-`l` components before further equivariant processing.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_scalar_tensor(x: torch.Tensor, lmax: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Expand scalar tensor to spherical harmonics shape with angular momentum up to `lmax`\n",
    "\n",
    "    Args:\n",
    "        x: tensor of shape [N_atoms, N_features]\n",
    "        lmax: maximum angular momentum\n",
    "\n",
    "    Returns:\n",
    "        zero-padded tensor to shape [N_atoms, (lmax+1)^2*N_features]\n",
    "    \"\"\"\n",
    "    x = x.unsqueeze(1)\n",
    "    \n",
    "    y = torch.cat(\n",
    "        [\n",
    "            x,\n",
    "            torch.zeros(\n",
    "                (x.shape[0], int((lmax + 1) ** 2 - 1), x.shape[2]),\n",
    "                device=x.device,\n",
    "                dtype=x.dtype,\n",
    "            ),\n",
    "        ],\n",
    "        dim=1,\n",
    "    )\n",
    "    return torch.reshape(y,(x.shape[0],(lmax+1)**2*x.shape[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple `SO(3)`-equivariant neural network\n",
    "\n",
    "This model implements a basic `SO(3)`-equivariant message passing network using irreducible representations.\n",
    "\n",
    "The architecture:\n",
    "\n",
    "- Embeds atoms as scalars (`l=0`) and expands them to match a full SO(3) irrep structure (via `expand_scalar_tensor`)\n",
    "- Represents directional edge information via spherical harmonics and RBFs\n",
    "- Applies several rounds of equivariant convolutions (with `l ≤ lmax`)\n",
    "- Predicts a vector-valued property (e.g. molecular dipole) using an equivariant linear readout (`l=1`)\n",
    "\n",
    "<details>\n",
    "<summary><strong>Hint</strong></summary>\n",
    "    \n",
    " Why `1x1o` for the readout?\n",
    "\n",
    "Dipole moments are 3D vectors. In `SO(3)` terms, these transform like `l=1` irreps with odd parity (`1o`).\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleEqNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple SO3-equivariant representation \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dimension,\n",
    "        lmax,\n",
    "        n_interactions, \n",
    "        radial_basis,\n",
    "        n_elements, \n",
    "        activation = F.silu\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            feature_dimension: number of features to describe atomic environments.\n",
    "                This determines the size of each embedding vector; i.e. embeddings_dim.\n",
    "            n_interactions: number of interaction blocks.\n",
    "            lmax: maximum angular momentum of spherical harmonics basis\n",
    "            radial_basis: layer for expanding interatomic distances in a basis set\n",
    "            activation:\n",
    "        \"\"\"\n",
    "        super(SimpleEqNet, self).__init__()\n",
    "\n",
    "        self.feature_dimension = feature_dimension\n",
    "        self.lmax = lmax\n",
    "        self.hidden_irreps = o3.Irreps(f\"{self.feature_dimension}x0e+{self.feature_dimension}x1o\") # works only for lmax = 1\n",
    "        self.n_interactions = n_interactions\n",
    "        self.radial_basis = radial_basis\n",
    "        self.edge_dimension = radial_basis.n_rbf\n",
    "        self.activation = activation\n",
    "        \n",
    "        # TODO: initialize embeddings\n",
    "        \n",
    "        # TODO: initialize equivariant convolution\n",
    "\n",
    "        # TODO: we are predicting eq features. Pick only 1x1o atomic vectors, and do a scatter sum to get a cluster dipole from them with linear function\n",
    "\n",
    "\n",
    "    def forward(self, inputs: BatchedTorchAtoms):\n",
    "        \"\"\"\n",
    "        Compute atomic representations/embeddings.\n",
    "\n",
    "        Args:\n",
    "            inputs (dict of torch.Tensor): torchAtoms with input tensors.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: atom-wise representation.\n",
    "        \"\"\"\n",
    "        # get tensors from input \n",
    "        species_types = inputs.element_labels\n",
    "        r_ij = inputs.r_ij\n",
    "        d_ij = inputs.d_ij\n",
    "        dir_ij = inputs.dir_ij\n",
    "        idx_i = inputs.idx_i\n",
    "        idx_j = inputs.idx_j\n",
    "\n",
    "        # Radial embedding\n",
    "        e_ij = self.radial_basis(d_ij)\n",
    "\n",
    "        # Node embedding\n",
    "        x = self.embedding(species_types)\n",
    "        x = expand_scalar_tensor(x,self.lmax)\n",
    "\n",
    "        # Message passing of eqconv and pooling\n",
    "        \n",
    "        # Read out\n",
    "        dipoles = self.dipoles_read(x)\n",
    "        # I have took this readout function from MACE, so I am leaving it here\n",
    "        # total_dipole = scatter_sum( # stolen from mace\n",
    "        #     src=dipoles,\n",
    "        #     index=inputs.mol_id,\n",
    "        #     dim=0,\n",
    "        #     dim_size=inputs.n_mols,\n",
    "        # )\n",
    "        # return total_dipole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training your first equivariant NN\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dipole prediction with equivariant GNN**\n",
    "\n",
    "We now train the equivariant network to predict the **total molecular dipole**  \n",
    "from atom-wise representations, aggregated via equivariant readout.\n",
    "\n",
    "Now, build the training loop by correctly passing the reference dipole into the loss function.  \n",
    "\n",
    "<details>\n",
    "<summary><strong> Hint</strong></summary>\n",
    "\n",
    "Use `loss = F.mse_loss(dip, reference_dipole)`, since we want to train on the dipoles.\n",
    "\n",
    "The current setup processes a single molecule as one batch.  \n",
    "For multiple systems, per-molecule dipoles would need to be computed and compared individually - cf. the batched invariant NN training above.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increasing the body order: Tensor Product, Mixing and Gating\n",
    "\n",
    "To move from a simple equivariant message passing network to a **fully expressive SO(3)Net-like architecture**,  \n",
    "we need to increase the **body order** and model more complex local geometric interactions.\n",
    "\n",
    "This is achieved through the following additions:\n",
    "\n",
    "1. A **mixing layer** that prepares node features for interaction\n",
    "2. A **tensor product** that increases the body order (e.g. from 2-body to 3-body terms)\n",
    "3. A **gating mechanism** that enables non-linear interactions for higher-order irreps\n",
    "4. Additional **mixing layers** before and after gating\n",
    "\n",
    "<details>\n",
    "<summary><strong>Architecture overview</strong> (click to expand)</summary>\n",
    "\n",
    "Each interaction block now proceeds through:\n",
    "\n",
    "- **Equivariant convolution**: Couples neighbors via direction-aware message passing\n",
    "- **Linear mixing**: Prepares node features for interaction\n",
    "- **Tensor product**: Increases local body order (e.g. captures angular correlations)\n",
    "- **Gate**: Applies scalar-controlled nonlinearity to non-scalar features\n",
    "- **Residual update**: Adds processed message to current node state\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "    <summary><strong>Hint</strong></summary>\n",
    "\n",
    "What is “body order”?\n",
    "\n",
    "The body order describes how many atoms interact simultaneously in a feature.  \n",
    "E.g., standard message passing is 2-body (central atom + neighbor),  \n",
    "while tensor product can model 3-body or higher interactions (center + 2 neighbors, angular structures).\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><strong>Hint</strong></summary>\n",
    "    \n",
    "What does the gate do?\n",
    "\n",
    "Scalars are passed through a nonlinearity (e.g. sigmoid) and used to modulate the amplitudes  \n",
    "of higher-order (vector/tensor) components. This enables non-linear equivariant processing.\n",
    "\n",
    "</details>\n",
    "\n",
    "This network retains full SO(3)-equivariance and is expressive enough for vector- and tensor-valued predictions.  \n",
    "Here, the readout produces a dipole vector (`1×1o`) as the final molecular property.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final remarks: This is a final solution with everything. If you look at the bottom of this notebook, I left there the solution for this. If you will be completly lost, you can go there and have a look. You can in theory also copy it and play with adding interaction, linear blocks, making smaller or larger dimensions of a feature blocks. In theory you can expand hidden irreps to larger $\\ell$ or spherical harmonics in convolution.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training your fancy equivariant NN\n",
    "\n",
    "We now train the full SO(3)-equivariant network to predict the molecular **dipole vector**  \n",
    "based on the atom-wise learned features and directional message passing.\n",
    "\n",
    "Your job: Provide a complete training loop once more.\n",
    "\n",
    "<details>\n",
    "    <summary><strong>Hint</strong></summary>\n",
    "    \n",
    "    You'll have to \n",
    "- get `n_elements`\n",
    "- set `lmax` and `feature_dimension`\n",
    "- instantiate the an `EqNet` object\n",
    "- read molecule data and use `TorchAtoms` resp. `BatchedTorchAtoms`\n",
    "- make sure you have the training information (`ref_dipole`) as a correctly shaped `torch.tensor`\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, after setting up everything, get the optimizer running in a training `for`-loop.\n",
    "\n",
    "<details>\n",
    "    <summary><strong>Hint</strong></summary>\n",
    "    Basically the same as for the invariant network.\n",
    "    \n",
    "    Call the `forward`-method of your `EqNet`-object, calculate loss and don't forget to handle the gradient correctly!\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally again, plot as a scatter plot once more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotation equivariance test\n",
    "\n",
    "To verify that our network is truly `SO(3)`-equivariant, we compare its output on a molecule before and after a rigid rotation.\n",
    "\n",
    "Define a `rotation` function using `numpy.ndarray`'s.\n",
    "\n",
    "<details>\n",
    "<summary><strong>Hint</strong> What should hold if the network is equivariant?</summary>\n",
    "\n",
    "The predicted dipole after rotation should match the **rotated version** of the original prediction.\n",
    "    \n",
    "Steps:\n",
    "\n",
    "- Select a molecule (`i_mol`)\n",
    "- Predict dipole from original structure\n",
    "- Rotate the molecule around a chosen axis\n",
    "- Predict dipole again and compare with the rotated reference dipole\n",
    "\n",
    "</details>\n",
    "<br>\n",
    "\n",
    "Afterwards, take a molecule and use your `EqNet`-object. Test before and after rotation - what should happen? And what happens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def rotate_vector(v, axis, angle_rad):\n",
    "    axis = axis / np.linalg.norm(axis)  # normalize rotation axis\n",
    "    cos_theta = np.cos(angle_rad)\n",
    "    sin_theta = np.sin(angle_rad)\n",
    "    one_minus_cos = 1 - cos_theta\n",
    "    x, y, z = axis\n",
    "\n",
    "    # Rodrigues' rotation matrix\n",
    "    R = np.array([\n",
    "        [cos_theta + x*x*one_minus_cos,      x*y*one_minus_cos - z*sin_theta, x*z*one_minus_cos + y*sin_theta],\n",
    "        [y*x*one_minus_cos + z*sin_theta,    cos_theta + y*y*one_minus_cos,   y*z*one_minus_cos - x*sin_theta],\n",
    "        [z*x*one_minus_cos - y*sin_theta,    z*y*one_minus_cos + x*sin_theta, cos_theta + z*z*one_minus_cos  ]\n",
    "    ])\n",
    "\n",
    "    return R @ v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "i_mol = 9\n",
    "\n",
    "mol_orig = deepcopy(mols[i_mol])\n",
    "pred_orig_vector = gnn(BatchedTorchAtoms([TorchAtoms(mol_orig)])).tolist()\n",
    "rot_vec = rotate_vector(np.array(ref_dipole[i_mol,:]),(0,1,0),np.radians(32))\n",
    "mol_rots = deepcopy(mols[i_mol])\n",
    "mol_rots.rotate(32,(0,1,0))\n",
    "pred_rot_vector = gnn(BatchedTorchAtoms([TorchAtoms(mol_rots)])).tolist()\n",
    "\n",
    "print(ref_dipole[i_mol,:])\n",
    "print(pred_orig_vector)\n",
    "\n",
    "print(rot_vec)\n",
    "print(pred_rot_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations – you just trained your first fully SO(3)-equivariant neural network!\n",
    "\n",
    "You've now built a message-passing model that respects 3D geometry **by design**:  \n",
    "From scalar embeddings to equivariant convolutions, tensor products and gating –  \n",
    "this architecture forms the basis for state-of-the-art models in molecular and materials machine learning.\n",
    "\n",
    "<details>\n",
    "<summary><strong>What’s next?</strong> (click me)</summary>\n",
    "\n",
    "- Predict forces, multipoles or energies using the same architecture  \n",
    "- Explore higher `lmax` values or more interaction layers  \n",
    "- Train on diverse molecular datasets (QM9, MD17, SPICE, ...)  \n",
    "- Benchmark against SchNet, DimeNet, or GemNet\n",
    "\n",
    "</details>\n",
    "\n",
    "_Equivariance is not a trick – it's a symmetry principle. And you’ve just made it learnable._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EqNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A more involved SO3-equivariant representation \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dimension,\n",
    "        lmax,\n",
    "        n_interactions, \n",
    "        radial_basis,\n",
    "        n_elements, \n",
    "        activation = F.silu\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            feature_dimension: number of features to describe atomic environments.\n",
    "                This determines the size of each embedding vector; i.e. embeddings_dim.\n",
    "            n_interactions: number of interaction blocks.\n",
    "            lmax: maximum angular momentum of spherical harmonics basis\n",
    "            radial_basis: layer for expanding interatomic distances in a basis set\n",
    "            activation:\n",
    "        \"\"\"\n",
    "        super(EqNet, self).__init__()\n",
    "\n",
    "        self.feature_dimension = feature_dimension\n",
    "        self.lmax = lmax\n",
    "        self.hidden_irreps = o3.Irreps(f\"{self.feature_dimension}x0e+{self.feature_dimension}x1o\") # works only for lmax = 1\n",
    "        self.n_interactions = n_interactions\n",
    "        self.radial_basis = radial_basis\n",
    "        self.edge_dimension = radial_basis.n_rbf\n",
    "        self.activation = activation\n",
    "        \n",
    "        # initialize embeddings\n",
    "        self.embedding = nn.Embedding(n_elements, feature_dimension)\n",
    "\n",
    "        # initialize linear mixing layers\n",
    "        # self.linear = nn.Linear(feature_dimension, feature_dimension, bias=False)\n",
    "\n",
    "        # initialize equivariant convolution\n",
    "        self.equconv  = EquivariantConvolution(self.lmax,\n",
    "                                               self.feature_dimension,\n",
    "                                               self.edge_dimension,\n",
    "                                               )\n",
    "        \n",
    "        # initialize mixing before the tensor product\n",
    "        self.mixing_to_tp = o3.Linear(irreps_in=self.hidden_irreps,irreps_out=self.hidden_irreps)\n",
    "\n",
    "        # initialize the tensor product\n",
    "        self.tp = o3.FullyConnectedTensorProduct(irreps_in1=self.hidden_irreps, irreps_in2=self.hidden_irreps, irreps_out=self.hidden_irreps,internal_weights=False)\n",
    "        \n",
    "        # separate the scalar features from the non-scalar features based on Irrep.l\n",
    "        irreps_scalars = o3.Irreps(\n",
    "            [(mul, ir) for mul, ir in self.hidden_irreps if ir.l == 0]\n",
    "        )\n",
    "        irreps_gated = o3.Irreps(\n",
    "            [(mul, ir) for mul, ir in self.hidden_irreps if ir.l > 0]\n",
    "        )\n",
    "        \n",
    "        # define an additionally needed amount of scalar features\n",
    "        irreps_gates = o3.Irreps([mul, \"0e\"] for mul, _ in irreps_gated)\n",
    "\n",
    "        # initialize the gating\n",
    "        self.gating = Gate(\n",
    "            irreps_scalars=irreps_scalars,\n",
    "            act_scalars=[torch.sigmoid],\n",
    "            irreps_gates=irreps_gates,\n",
    "            act_gates=[torch.sigmoid],\n",
    "            irreps_gated=irreps_gated\n",
    "        )\n",
    "\n",
    "        # define the total amount of irreps for the nonlinearity \n",
    "        self.irreps_nonlin = self.gating.irreps_in.simplify()\n",
    "\n",
    "        # initialize mixing layers before and after the gating\n",
    "        self.mixing_to_gate = o3.Linear(irreps_in=self.hidden_irreps,irreps_out=self.irreps_nonlin)\n",
    "        self.mixing_to_read = o3.Linear(irreps_in=self.hidden_irreps,irreps_out=self.hidden_irreps)\n",
    "\n",
    "        # initialize the readout\n",
    "        self.dipole_out = o3.Irreps(\"1x1o\")\n",
    "        self.dipoles_read = o3.Linear(irreps_in=self.hidden_irreps, irreps_out=self.dipole_out)\n",
    "\n",
    "\n",
    "    def forward(self, inputs: BatchedTorchAtoms):\n",
    "        \"\"\"\n",
    "        Compute atomic representations/embeddings.\n",
    "\n",
    "        Args:\n",
    "            inputs (dict of torch.Tensor): torchAtoms with input tensors.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: atom-wise representation.\n",
    "        \"\"\"\n",
    "        # get tensors from input \n",
    "        species_types = inputs.element_labels\n",
    "        r_ij = inputs.r_ij\n",
    "        d_ij = inputs.d_ij\n",
    "        dir_ij = inputs.dir_ij\n",
    "        idx_i = inputs.idx_i\n",
    "        idx_j = inputs.idx_j\n",
    "\n",
    "        # Radial embedding\n",
    "        e_ij = self.radial_basis(d_ij)\n",
    "\n",
    "        # Node embedding\n",
    "        x = self.embedding(species_types)\n",
    "        x = expand_scalar_tensor(x,self.lmax)\n",
    "\n",
    "        for t in range(self.n_interactions):\n",
    "            # Apply convolution\n",
    "            dx = self.equconv(x, e_ij, r_ij, idx_i, idx_j)\n",
    "            # Prepare the features for tensor product\n",
    "            ddx = self.mixing_to_tp(dx)\n",
    "            # Apply the tensor product\n",
    "            dx = dx + self.tp(dx, ddx,torch.ones(self.tp.weight_numel))\n",
    "            # Prepare the features for the gating\n",
    "            dx = self.mixing_to_gate(dx)\n",
    "            # Apply the gating\n",
    "            dx = self.gating(dx)\n",
    "            # Recover original dimension\n",
    "            dx = self.mixing_to_read(dx)\n",
    "            # Update node representation\n",
    "            x = x + dx\n",
    "        dipoles = self.dipoles_read(x)\n",
    "        # print(\"dipoles\",dipoles.shape)\n",
    "        total_dipole = scatter_sum( # stolen from mace\n",
    "            src=dipoles,\n",
    "            index=inputs.mol_id,\n",
    "            dim=0,\n",
    "            dim_size=inputs.n_mols,\n",
    "        )\n",
    "        return total_dipole"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.9.19",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
